import json
import os

import openai

from dotenv import load_dotenv

from apo import ChatGPT as llm
from apo import MessageTemplate


load_dotenv()
openai.api_key = os.environ["OPENAI_API_KEY"]


async def generate_gradients(prompt: str, error_str: str, num_feedbacks: int = 3, **openai_kwargs) -> str:
    """
    Asynchronously generate synthetic gradients for a given language prompt based on a specified error string.

    These gradients are textual feedback that point out the flaws or errors in the original prompt.

    Parameters
    ----------
    prompt : str
        The initial natural language prompt that is under optimization.
        This is the string for which the function will generate synthetic gradients to improve it.

    error_str : str
        A string that encapsulates the errors or issues with the predictions generated by the language model when
        using the initial prompt. This string is used by the language model to identify weaknesses in the original prompt.

    num_feedbacks : int, optional
        The number of synthetic gradients to generate. These are essentially pieces of feedback pointing out the issues
        or possible improvements for the original prompt. Default value is 3.

    **openai_kwargs : dict
        Additional keyword arguments that can be passed to the OpenAI API call.
        This allows for customization like setting the engine, max tokens, temperature, etc.

    """
    generate_gradients = MessageTemplate.load("src/apo/prompts/gradient_descent/generate_gradients/user.json")
    generate_gradients.format_message(prompt=prompt, error_string=error_str, num_feedbacks=num_feedbacks)
    messages = [generate_gradients.to_prompt()]
    response = await llm.generate(messages=messages, **openai_kwargs)
    gradients = response["content"]
    gradients_as_list = [reason["reason"] for reason in json.loads(gradients)["reasons"]]
    return "\n".join(gradients_as_list)


async def edit_prompt_with_gradients(
    prompt: str, error_str: str, gradients: str, steps_per_gradient: int = 3, **openai_kwargs
) -> str:
    """
    Asynchronously edit and optimize a given natural language prompt based on provided synthetic gradients.

    This function uses OpenAI's API to iteratively improve the promptbased on the weaknesses identified by
    the synthetic gradients.

    Parameters
    ----------
    prompt : str
        The initial natural language prompt that needs to be optimized.
        This is the prompt for which the function will apply edits to improve its effectiveness.

    error_str : str
        A string that encapsulates the errors or shortcomings in the predictions generated by the language model
        when using the initial prompt. This string is used to provide context for the editing process.

    gradients : str
        A string containing synthetic gradients for the prompt. These gradients are textual feedback that point out
        flaws or potential areas for improvement in the original prompt.

    steps_per_gradient : int, optional
        The number of editing steps to take for each synthetic gradient. This controls how aggressively the prompt
        is edited. Default value is 3.

    **openai_kwargs : dict
        Additional keyword arguments that can be passed to the OpenAI API call.
        These could be settings like engine type, max tokens, temperature, etc.
    """

    edit_prompt = MessageTemplate.load("src/apo/prompts/gradient_descent/edit_prompt_w_gradient/user.json")
    edit_prompt.format_message(
        prompt=prompt, error_string=error_str, gradients=gradients, steps_per_gradient=steps_per_gradient
    )
    messages = [edit_prompt.to_prompt()]
    response = await llm.generate(messages=messages, **openai_kwargs)
    return response["content"]
