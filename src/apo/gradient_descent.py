import asyncio
import json
import os

from asyncio import Semaphore
from typing import Dict
from typing import List

import evaluate
import openai
import pandas as pd

from dotenv import load_dotenv

from apo import ChatGPT as llm
from apo import MessageTemplate


load_dotenv()
openai.api_key = os.environ["OPENAI_API_KEY"]


async def generate_gradients(prompt: str, error_str: str, num_feedbacks: int = 3, **openai_kwargs) -> str:
    """
    Asynchronously generate synthetic gradients for a given language prompt based on a specified error string.

    These gradients are textual feedback that point out the flaws or errors in the original prompt.

    Parameters
    ----------
    prompt : str
        The initial natural language prompt that is under optimization.
        This is the string for which the function will generate synthetic gradients to improve it.

    error_str : str
        A string that encapsulates the errors or issues with the predictions generated by the language model when
        using the initial prompt. This string is used by the language model to identify weaknesses in the original prompt.

    num_feedbacks : int, optional
        The number of synthetic gradients to generate. These are essentially pieces of feedback pointing out the issues
        or possible improvements for the original prompt. Default value is 3.

    **openai_kwargs : dict
        Additional keyword arguments that can be passed to the OpenAI API call.
        This allows for customization like setting the engine, max tokens, temperature, etc.

    """
    generate_gradients = MessageTemplate.load("src/apo/prompts/gradient_descent/generate_gradients/user.json")
    generate_gradients.format_message(prompt=prompt, error_string=error_str, num_feedbacks=num_feedbacks)
    messages = [generate_gradients.to_prompt()]
    response = await llm.generate(messages=messages, **openai_kwargs)
    gradients = response["content"]
    gradients_as_list = [reason["reason"] for reason in json.loads(gradients)["reasons"]]
    return "\n".join(gradients_as_list)


async def edit_prompt_with_gradients(
    prompt: str, error_str: str, gradients: str, steps_per_gradient: int = 3, **openai_kwargs
) -> str:
    """
    Asynchronously edit and optimize a given natural language prompt based on provided synthetic gradients.

    This function uses OpenAI's API to iteratively improve the promptbased on the weaknesses identified by
    the synthetic gradients.

    Parameters
    ----------
    prompt : str
        The initial natural language prompt that needs to be optimized.
        This is the prompt for which the function will apply edits to improve its effectiveness.

    error_str : str
        A string that encapsulates the errors or shortcomings in the predictions generated by the language model
        when using the initial prompt. This string is used to provide context for the editing process.

    gradients : str
        A string containing synthetic gradients for the prompt. These gradients are textual feedback that point out
        flaws or potential areas for improvement in the original prompt.

    steps_per_gradient : int, optional
        The number of editing steps to take for each synthetic gradient. This controls how aggressively the prompt
        is edited. Default value is 3.

    **openai_kwargs : dict
        Additional keyword arguments that can be passed to the OpenAI API call.
        These could be settings like engine type, max tokens, temperature, etc.
    """

    edit_prompt = MessageTemplate.load("src/apo/prompts/gradient_descent/edit_prompt_w_gradient/user.json")
    edit_prompt.format_message(
        prompt=prompt, error_string=error_str, gradients=gradients, steps_per_gradient=steps_per_gradient
    )
    messages = [edit_prompt.to_prompt()]
    response = await llm.generate(messages=messages, **openai_kwargs)
    return response["content"]


async def run_prompt(semaphore: Semaphore, prompt: MessageTemplate, prompt_args: Dict, **openai_kwargs) -> str:
    """Asynchronously evaluate a MessageTemplate prompt and return the output"""
    async with semaphore:
        prompt.format_message(**prompt_args)
        messages = [prompt.to_prompt()]
        response = await llm.generate(messages=messages, **openai_kwargs)
        return response["content"]


def compute_accuracy(predictions: pd.Series, labels: pd.Series) -> float:
    """Compute accuracy of predictions"""
    return (predictions == labels).sum() / len(predictions)


def evaluate_predictions(predictions: List, labels: List, metric: str) -> float:
    """
    Evaluate the model's predictions using various metrics.

    Parameters:
    -----------
    predictions : pd.Series
        Series object containing the model's predictions.

    labels : pd.Series
        Series object containing the ground-truth labels.

    metric : str
        The type of metric to use for evaluation. Supported types are "accuracy", "f1", "recall", "rouge", and "bleu".

    Returns:
    --------
    result : float or dict
        The calculated metric value. For some metrics like 'rouge', a dictionary of values will be returned.
    """

    # Map metric names to metric functions
    metric_fn = evaluate.load(metric)

    # Compute the metric
    if metric in ["accuracy", "f1", "recall"]:
        result = metric_fn.compute(predictions=predictions, references=labels)
    elif metric == "rouge":
        result = metric_fn.compute(
            predictions=predictions, references=labels, rouge_types=["rouge1", "rouge2", "rougeL"]
        )
    elif metric == "bleu":
        result = metric_fn.compute(predictions=[predictions], references=[[labels]])
    else:
        raise ValueError(f"Unsupported metric: {metric}")

    return result


async def evaluate_prompt(
    prompt: MessageTemplate,
    data: pd.DataFrame,
    input_cols: List,
    label_col: str = "label",
    concurrency: int = 10,
    metric: str = "accuracy",
    label_mapping: Dict = None,
    **openai_kwargs,
) -> float:
    """
    Evaluate a MessageTemplate prompt using a given dataset.

    Parameters:
    -----------
    prompt : MessageTemplate
        The MessageTemplate object to evaluate.

    data : pd.DataFrame
        The dataset to use for evaluation.

    input_cols : List
        The list of input columns to use for evaluation, only keep the columns that are required for the prompt.

    label_col : str
        The name of the column containing the ground-truth labels.
    """

    inputs = data[input_cols].to_dict("records")

    semaphore = asyncio.Semaphore(concurrency)
    tasks = [run_prompt(semaphore, prompt, prompt_input, **openai_kwargs) for prompt_input in inputs]

    results = await asyncio.gather(*tasks)

    if label_mapping is not None:
        results = [label_mapping[result] for result in results]

    return evaluate_predictions(results, data[label_col].tolist(), metric)
